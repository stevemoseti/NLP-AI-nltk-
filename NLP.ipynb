{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4AIHFgSd53bFLCAdCJM70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevemoseti/NLP-AI-nltk-/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F07dxdUDQfut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f992489-be2d-4460-9f12-831a370fe234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# NLTK word tokenize or sent_tokenize\n",
        "#imports\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install pandas\n",
        "import pandas as pd  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence = \"But there's provision for police to kill innocent unarmed luos giving given gave me. We luos we need to protect ourselves otherwise we will be killed despite being innocent\""
      ],
      "metadata": {
        "id": "nuRSlk5cVYZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZATION\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "tokens = (nltk.word_tokenize(sentence)) #this is for word totenization\n",
        "# print(nltk.sent_tokenize(sentence)) #sentence tokenization"
      ],
      "metadata": {
        "id": "mouplwsaV1ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-73PDdoY3mz",
        "outputId": "35afba08-1fef-475e-fdf0-d9c417047f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['But',\n",
              " 'there',\n",
              " \"'s\",\n",
              " 'provision',\n",
              " 'for',\n",
              " 'police',\n",
              " 'to',\n",
              " 'kill',\n",
              " 'innocent',\n",
              " 'unarmed',\n",
              " 'luos',\n",
              " 'giving',\n",
              " 'given',\n",
              " 'gave',\n",
              " 'me',\n",
              " '.',\n",
              " 'We',\n",
              " 'luos',\n",
              " 'we',\n",
              " 'need',\n",
              " 'to',\n",
              " 'protect',\n",
              " 'ourselves',\n",
              " 'otherwise',\n",
              " 'we',\n",
              " 'will',\n",
              " 'be',\n",
              " 'killed',\n",
              " 'despite',\n",
              " 'being',\n",
              " 'innocent']"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COUNTING NO OF WORDS\n",
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzprbcYMXOXD",
        "outputId": "56d7c0c6-8908-4eab-fd31-c84e86aca6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FREQUENT WORDS DISTRIBUTION\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()\n",
        "\n",
        "for word in tokens:\n",
        "  fdist[word.lower()]+=1\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYwqaWESZCWq",
        "outputId": "aa579000-f3a6-4a6b-96c1-f9782637046b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({\"'s\": 1,\n",
              "          '.': 1,\n",
              "          'be': 1,\n",
              "          'being': 1,\n",
              "          'but': 1,\n",
              "          'despite': 1,\n",
              "          'for': 1,\n",
              "          'gave': 1,\n",
              "          'given': 1,\n",
              "          'giving': 1,\n",
              "          'innocent': 2,\n",
              "          'kill': 1,\n",
              "          'killed': 1,\n",
              "          'luos': 2,\n",
              "          'me': 1,\n",
              "          'need': 1,\n",
              "          'otherwise': 1,\n",
              "          'ourselves': 1,\n",
              "          'police': 1,\n",
              "          'protect': 1,\n",
              "          'provision': 1,\n",
              "          'there': 1,\n",
              "          'to': 2,\n",
              "          'unarmed': 1,\n",
              "          'we': 3,\n",
              "          'will': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING HOW MANY TIMES A WORD \n",
        "fdist['luos']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OnUvuHia3ea",
        "outputId": "d7f42dc7-2ce0-436f-8459-e5daf289f4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOP 3 MOST FREQUENT WORDS\n",
        "top3 = fdist.most_common(3)\n",
        "top3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APTMkn3JbLM6",
        "outputId": "5c51fbb2-a003-4143-a4b1-a1f94871647a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we', 3), ('to', 2), ('innocent', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BREAKING IN N-GRAMS(breaking into 2 words,3 words etc)\n",
        "text = \" i love this lady\"\n",
        "sent = nltk.word_tokenize(text)\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqFenr6lcBpC",
        "outputId": "40b8a05d-0928-4fbd-9cc2-9424a9f44400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'this', 'lady']"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams,trigrams, ngrams\n",
        "sentbigrams = list(nltk.bigrams(sent))\n",
        "sentbigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVdjZtewdkaT",
        "outputId": "159ff70a-bdbe-463c-d8a4-8ba923192b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'love'), ('love', 'this'), ('this', 'lady')]"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "senttrigrams = list(nltk.trigrams(sent))\n",
        "senttrigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3jYqcF8eQYT",
        "outputId": "a86cc581-81af-4cfe-b94d-39ab7a3a6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'love', 'this'), ('love', 'this', 'lady')]"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentngrams = list(nltk.ngrams(sent,4))\n",
        "sentngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qol6JMr3e1ff",
        "outputId": "9da8879c-9176-42df-f924-3ff6ab75f4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'love', 'this', 'lady')]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING portstemmer is the most famous one that returns words to there root words\n",
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer() #inistatization\n"
      ],
      "metadata": {
        "id": "NeoWzMRrfLWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the already tokenized worda in sentence lets do stemming\n",
        "for words in tokens:\n",
        "  print(words+ \":\"+pst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0wHyXxugLjS",
        "outputId": "5d6267a5-4b2a-445a-ed33-b1a81e57219c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "But:but\n",
            "there:there\n",
            "'s:'s\n",
            "provision:provis\n",
            "for:for\n",
            "police:polic\n",
            "to:to\n",
            "kill:kill\n",
            "innocent:innoc\n",
            "unarmed:unarm\n",
            "luos:luo\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n",
            "me:me\n",
            ".:.\n",
            "We:We\n",
            "luos:luo\n",
            "we:we\n",
            "need:need\n",
            "to:to\n",
            "protect:protect\n",
            "ourselves:ourselv\n",
            "otherwise:otherwis\n",
            "we:we\n",
            "will:will\n",
            "be:be\n",
            "killed:kill\n",
            "despite:despit\n",
            "being:be\n",
            "innocent:innoc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#another one in LancasterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()"
      ],
      "metadata": {
        "id": "WOTB6-KQhXcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lancasterstemmer stores data into four entries it eliminates the last words \n",
        "for words in tokens:\n",
        "  print(words+\":\"+lst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwR1go5EhokD",
        "outputId": "493587de-fd83-4941-8501-3567eb490aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "But:but\n",
            "there:ther\n",
            "'s:'s\n",
            "provision:provid\n",
            "for:for\n",
            "police:pol\n",
            "to:to\n",
            "kill:kil\n",
            "innocent:innoc\n",
            "unarmed:unarm\n",
            "luos:luo\n",
            "giving:giv\n",
            "given:giv\n",
            "gave:gav\n",
            "me:me\n",
            ".:.\n",
            "We:we\n",
            "luos:luo\n",
            "we:we\n",
            "need:nee\n",
            "to:to\n",
            "protect:protect\n",
            "ourselves:ourselv\n",
            "otherwise:otherw\n",
            "we:we\n",
            "will:wil\n",
            "be:be\n",
            "killed:kil\n",
            "despite:despit\n",
            "being:being\n",
            "innocent:innoc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LEMMATIZATION group together its more like stemming but  it converts words into proper words eg gone , going and went => go\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import wordnet #word dictionary\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer() #instatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z11bmcgEiSto",
        "outputId": "06a3929a-7303-42b1-a265-4d03d72f61e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_lem.lemmatize(\"corpora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CT3ypEzjjBkk",
        "outputId": "f4d1f538-1c20-4b9d-da9c-8d9bbddf573a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in tokens:\n",
        "  print(words+ \": \"+word_lem.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toT4BZ-pjwIO",
        "outputId": "63d38d7c-2d12-4a97-c8d8-54dd4294e64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "But: But\n",
            "there: there\n",
            "'s: 's\n",
            "provision: provision\n",
            "for: for\n",
            "police: police\n",
            "to: to\n",
            "kill: kill\n",
            "innocent: innocent\n",
            "unarmed: unarmed\n",
            "luos: luo\n",
            "giving: giving\n",
            "given: given\n",
            "gave: gave\n",
            "me: me\n",
            ".: .\n",
            "We: We\n",
            "luos: luo\n",
            "we: we\n",
            "need: need\n",
            "to: to\n",
            "protect: protect\n",
            "ourselves: ourselves\n",
            "otherwise: otherwise\n",
            "we: we\n",
            "will: will\n",
            "be: be\n",
            "killed: killed\n",
            "despite: despite\n",
            "being: being\n",
            "innocent: innocent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STOPWORDS REMOVAL i,me, all pronouns\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words(\"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7LOMaVXkPoJ",
        "outputId": "046620a9-28b3-4e01-d87b-37d1c231d73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFok3uTWlFyE",
        "outputId": "18349fd6-7be9-49f9-a80b-a5b27eb0ca61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stop words in luos sentence\n",
        "tokens_without_stopwords = [word for word in tokens if not word in stopwords.words('english')]\n",
        "print(tokens_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH6bIqvclL8S",
        "outputId": "e00e6e31-0463-46fd-8c02-d1955d6162d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['But', \"'s\", 'provision', 'police', 'kill', 'innocent', 'unarmed', 'luos', 'giving', 'given', 'gave', '.', 'We', 'luos', 'need', 'protect', 'otherwise', 'killed', 'despite', 'innocent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PARTS OF SPEECH\n",
        "sentence = \"The fear of the lord is the beginning of knowledge.\"\n",
        "sentence_tokenize = nltk.word_tokenize(sentence)\n",
        "sentence_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIOdgstcnPlr",
        "outputId": "31651983-7ae5-46c7-9ce0-2f8fb313cea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'fear',\n",
              " 'of',\n",
              " 'the',\n",
              " 'lord',\n",
              " 'is',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words in above\n",
        "tokens_without_stopwords = [word for word in sentence_tokenize if not word in stopwords.words('english')]\n",
        "(tokens_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQVHg1mEn1yk",
        "outputId": "a3f96591-70bf-4f19-a121-0513aa2d8131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'fear', 'lord', 'beginning', 'knowledge', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the parts of speech\n",
        "from nltk.tag import pos_tag\n",
        "tagged = nltk.pos_tag(sentence_tokenize)\n"
      ],
      "metadata": {
        "id": "AuPJQSbeqrXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DqqD9Q6rJQy",
        "outputId": "8abcb114-475d-4887-9901-7e6120118a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('fear', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lord', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('beginning', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('knowledge', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NAMED ENTITY RECOGNITION\n",
        "from nltk import ne_chunk"
      ],
      "metadata": {
        "id": "ZD_y1y_QrY4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The USA president Obama lives in WHITE HOUSE\" #how can a system know that white house is recognized entity and not a \"house painted white 😂😂😂\"\n",
        "senttokens = nltk.word_tokenize(sentence)\n",
        "NE_tags = nltk.pos_tag(senttokens)\n",
        "# NE_tags"
      ],
      "metadata": {
        "id": "RRCK2-RDrmXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ne_tree = nltk.ne_chunk(NE_tags)"
      ],
      "metadata": {
        "id": "isVh7o3ts-bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ne_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTuAWtTytX4S",
        "outputId": "9ca7708d-9d3b-492e-a507-5175c598bf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION USA/NNP)\n",
            "  president/NN\n",
            "  (PERSON Obama/NNP)\n",
            "  lives/VBZ\n",
            "  in/IN\n",
            "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHUNKING bringing together now after seperation its the opposite of tokenization\n",
        "# sentence =\"The big cate ate the little mouse that was under the fridge\"\n",
        "# new_tokens = nltk.word_tokenize(sentence)\n",
        "# pos_new_tokens = nltk.pos_tag(new_tokens)\n",
        "# newChunk= nltk.ne_chunk(pos_new_tokens)\n",
        "# newChunk"
      ],
      "metadata": {
        "id": "SiMbjw6JxGEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}